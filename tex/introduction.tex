\section{Introduction}

Convolutional Neural Network (CNN) has become a state-of-the-art algorithm for a wide range of applications like image classification~\cite{simonyan2014very}\cite{he2015deep}, object detection~\cite{redmon2015you} and other image-based tasks. Compared with traditional hand-crafted-feature-based methods, CNN introduces a uniform model for different tasks and adjusts the model based on different training data set. Thus CNN can be adopted in various tasks and keeps high model accuracy.

But CNN is still not widely applied in real applications because of its high computation and memory complexity. A typical network like AlexNet~\cite{krizhevsky2012imagenet} consists of more than 240MB parameters and 1.4G FLOPs (floating-point operations) for the inference of a single $224\times 224$ image. More advanced networks~\cite{simonyan2014very, he2015deep} require much more computation and memory than AlexNet. The energy cost for CNN computation is thus high, especially on traditional platforms like CPU.

Various works explore energy efficient hardware designs for CNN accelerators. Many designs focus on CMOS based circuit design targeting at efficient data path and memory system~\cite{zhang2015optimizing, qiu2016going, du2015shidiannao}. In these designs, the size of the on-chip SRAM is quite limited. Thus external memory like DRAM is always needed in real applications. The high energy cost brought by off-chip data transfer dramatically limits the system energy efficiency.

One solution to reduce memory access is to perform in-memory computing. RRAM cross bar based design is one of the popular research topics. Chi et al. propose the PRIME~\cite{chi2016prime} architecture which implements the matrix-vector multiplication directly with the RRAM crossbar. Work by Cheng et al.~\cite{cheng2017time} implements the RRAM crossbar to support both inference and training of neural networks. In memory computation with RRAM shows attractive energy efficiency. But the application range of the design is limited by the scalability and computation accuracy of the RRAM crossbar.

Another way to reduce off-chip data transfer is to implement large on-chip memory, where RRAM is a good candidate. We compare RRAM with SRAM and DRAM in Table~\ref{tab:ram} according to the performance reported in \cite{ee598, fackenthal201419} and simulation result by NVSim~\cite{dong2014nvsim}. Compared with SRAM, the storage density of RRAM is similar to DRAM, which is about $20\times$ higher. Compared with DRAM, RRAM can be integrated on-chip while the former one can not.

\input{table/ram.tex}

But RRAM is also limited in some aspects. First, the I/O bandwidth of RRAM is smaller than SRAM. For applications like CPU cache, where latency is critical to performance, RRAM may not be a good choice. For CNN, the data access pattern is static. This means we can design data storage to achieve sequential access at run-time. So we can use more banks to compensate for the limited bandwidth.

Second, the I/O dynamic energy cost of RRAM is high In this paper, we show that a simple implementation with RRAM increases the total system energy cost. But there is a chance that we utilize the property of CNN computation to reduce on-chip memory access to overcome the energy overhead of using RRAM. 

In this paper, we introduce RRAM as the weight memory for a typical CNN accelerator design and optimize the design in hardware and scheduling level to overcome the RRAM energy overhead. The contributions of this paper are as follows:
\begin{itemize}
\item {Hardware optimization is proposed to reduce the RRAM dynamic energy overhead.}
\item {Dedicated scheduling strategy optimization is proposed to fully utilize the large RRAM buffer to reduce off-chip data transfer.}
\item {Design space exploration is done with state-of-the-art networks to show the effect of using RRAM as the on-chip buffer.}
\end{itemize}

Experimental results on state-of-the-art CNN models show that RRAM based design saves $12-18\%$ system energy with a $15-75\%$ smaller on-chip RAM area compared with SRAM design. The proposed hardware and scheduling optimization reduce up to 96\% on-chip RAM access energy and 98\% off-chip data transfer.

The rest of this paper is organized as follows: Section~\ref{sec:related} introduces the related work for CNN accelerator and RRAM research. Section~\ref{sec:hw} introduces the hardware design. The scheduling strategies are introduced in section~\ref{sec:schedule}. The experimental results are shown in section~\ref{sec:exp}. Section~\ref{sec:conclusion} concludes this paper.