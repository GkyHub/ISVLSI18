\section{Related Work}\label{sec:related}


\subsection{Convolutional Neural Network}
A CNN mainly consists of two types of layers, convolution (CNV) layers and fully connected (FC) layers, which contribute to most of the computation and parameters in a CNN. The computation for a CNV layer is shown as the pseudo code below. $F_{in}$ and $F_{out}$ are the input and output feature maps. $K$ and $b$ denote the convolution kernels and bias. The fully connected layer can be expressed as matrix-vector multiplication and can be treated as a special case of CNV layers where all the feature maps are $1\times 1$, and convolution kernels are $1\times 1$.

\begin{codebox}
\Procname{\proc{CnvLayer($F_{in}, F_{out}, K, b$)}}
\li \For {$m=1\to M$} \Comment output channel loop
  \Do
\li   \For {$n=1\to N$} \Comment Input channel loop
    \Do
\li  	\Comment feature map pixel loop
\li 	\For {each $F_{out}[m][x][y]\in F_{out}[m]$}
    \Do
\li   	  $F_{out}[m][x][y]=0$
\li 	  \Comment convolution kernel loop
\li 	  \For {each $K_{mn}[xx][yy]$}
          \Do
\li 		$F_{out}[m][x][y] += K_{mn}[xx][yy]*$\\
      $\qquad\qquad\qquad\qquad\qquad F_{in}[n][x+xx][y+yy] $
          \End
\li 	  $F_{out}[m][x][y]+=b_m$
        \End 
    \End
    \End 
\end{codebox}

Table~\ref{tab:cnn} shows the number of parameters and the computation complexity of some state-of-the-art network models. Usually a CNN consists of 10-100 million parameters and requires from 1-50G operations for the inference of each input. Compared with that, general purpose platforms like CPU only offers 1-100GOP/s computation capacity and consumes 1-100W power. This hardly meets the real-time requirement in video processing or the power limit of mobile platforms. There is urgent need of fast and energy efficient computation platform for processing CNN.

\input{table/cnn}

\subsection{CNN Accelerator}
The design of a CNN Accelerator involves two aspects: computation units and scheduling strategy. For computation units, various techniques have been studied to reduce the bit-width used for the data expression in CNN to reduce both the memory requirement and energy cost for computation and data transfer. Experimental results from~\cite{guo2017software} show that 8-bit linear data quantization introduces negligible accuracy loss for common CNN models compared with the original 32-bit floating-point model. Nonlinear quantization can further reduce the bit-width~\cite{han2015deep} but needs decoding logic and computation units with higher accuracy~\cite{eie}.

On scheduling aspect, most of the CNN accelerators~\cite{zhang2015optimizing}\cite{qiu2016going}\cite{ma2017optimizing} uses single layer scheduling strategies, where all the computation units work for the same layer at any time. Different layers are executed one by one. Single-layer design requires that the result of a layer to be written back to external memory when it is larger than the on-chip buffer. Alwani et al.~\cite{alwani2016fused} suggest that adjacent layers can be fused together as a scheduling unit to reduce the data transfer with the external memory.

As suggested by~\cite{mac_energy}, typical energy for a 32bit DRAM access is $100\times$ more than that of a 32-bit SRAM access, $200\times$ more than a 32bit multiplication and $6400\times$ more than a 32-bit addition. This indicates that the memory access energy contributes a major part of the total system cost. We also show this conclusion with our experiments in this paper.

\subsection{RRAM}
Metal-oxide resistive random access memory (RRAM) is one of the promising solutions for integrating large memory on-chip. The cell size of SRAM is typically 10-20x larger than that of RRAM with the same technology. Some memory designs~\cite{chien2009multi, chien2011multi} try to store multiple bits within a cell to further increase storage density. Despite the high storage density, the limited bandwidth of RRAM is always a consideration when using as on-chip memory. But recent work~\cite{fackenthal201419} shows that even high-density array of 32Gb achieves 1GB/s read bandwidth, and 200MB/s write bandwidth. In this work, we adopt~\cite{dong2014nvsim} as a tool to generate RRAM modules of different sizes in our experiments.

Using RRAM crossbar for both computation and storage for NN accelerator is another research topic~\cite{chi2016prime}\cite{cheng2017time}\cite{xia2016switched}. In these designs, extremely high energy efficieny is achieved because there is no energy needed to read network parameters from memory. But these designs are limited by the storage and computation accuracy of RRAM devices and the analog-digital converter overhead. In this paper, we only focus on using RRAM as on-chip memory, not as computation units.