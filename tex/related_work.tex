\section{Related Work}\label{sec:related}


\subsection{Convolutional Neural Network}
Convolutional neural networks(CNNs) mainly consist of two types of layers, convolution (CNV) layers and fully connected (FC) layers, which contribute to most of the computation and parameters in a CNN. The computation for a CNV layer is shown as the pseudo code below. $F_{in}$ and $F_{out}$ are the input and output feature maps. $K$ and $b$ denotes the convolution kernels and bias. The fully connected layer can be expressed as matrix vector multiplication and can be treated as a special case of CNV layers where all the feature maps are $1\times 1$ and convolution kernels are $1\times 1$.

\begin{codebox}
\Procname{\proc{CnvLayer($F_{in}, F_{out}, K, b$)}}
\li \Comment output channel loop
\li \For {$m=1\to M$} 
  \Do
\li   \Comment Input channel loop
\li   \For {$n=1\to N$}
    \Do
\li  	\Comment feature map pixel loop
\li 	\For {each $F_{out}[m][x][y]\in F_{out}[m]$}
    \Do
\li   	  $F_{out}[m][x][y]=0$
\li 	  \Comment convolution kernel loop
\li 	  \For {each $K_{mn}[xx][yy]$}
          \Do
\li 		$F_{out}[m][x][y] += K_{mn}[xx][yy]*$\\
      $\qquad\qquad\qquad\qquad\qquad F_{in}[n][x+xx][y+yy] $
          \End
\li 	  $F_{out}[m][x][y]+=b_m$
        \End 
    \End
    \End 
\end{codebox}

CNNs are both computation and memory intensive. Table~\ref{tab:cnn} shows the number of parameters and the computation complexity of some state-of-the-art network models. Usually a CNN consists of $10\sim 100$ million parameters and requires from $1\sim 50$G operations for the inference of each input. Compared with that, general purpose platforms like CPU only offers 1-100GOP/s computation capacity and consumes 1-100W power. This hardly meets the real-time requirement in video processing or the power limit of mobile platforms. There is urgent need of fast and energy efficient computation platform for processing CNN.

\input{table/cnn}

\subsection{CNN Accelerator}
The design of a CNN Accelerator involves two aspects: computation units and scheduling strategy. For computation units, various researches have been carried out to reduce the bitwidth used for the data expression in CNN as this reduce both the memory requirement and energy cost for computation and data transfer. Experimental results from~\cite{guo2017software} shows that 8-bit linear data quantization introduces negligible accuracy loss for common CNN models compared with the original 32-bit floating-point model. Nonlinear quantization can further reduce the bitwidth~\cite{han2015deep} but needs decoding logic and computation units with higher accuracy~\cite{eie};

On scheduling aspect, most of the CNN accelerators~\cite{zhang2015optimizing}\cite{qiu2016going}\cite{ma2017optimizing} uses single layer scheduling strategies, where all the computation units work for the same layer at any time. Different layers are executed one by one. As concluded in~\cite{ma2017optimizing}, one this kind of design should focus on three aspects: loop tiling, loop unrolling and loop interchange. Single layer design requires that the result of a layer to be written back to external memory when it is larger than the on-chip buffer. This is unnecessary because the result of one layer is the input of the next layer. Alwani, et al.~\cite{alwani2016fused} suggests that adjacent layers can be fused together as a schedule unit to reduce this overhead.

As suggested by~\cite{mac_energy}, typical energy for a 32bit DRAM access is $100\times$ more than that of a 32bit SRAM access, $200\times$ more than a 32bit multiplication and $6400\times$ more than a 32bit addition. This indicates that the memory access energy contributes a major part of the total system cost. Reducing data bitwidth will further increase the memory access cost as the energy for multiplication We also show this conclusion with our experiments later in this paper.

\subsection{RRAM}
Metal-oxide resistive random access memory (RRAM) is one of the promising solution for integrating large memory on-chip. The cell size of RRAM is typically $4\sim 6F^2$ while that for SRAM is more than $100F^2$. Some memory designs~\cite{chien2009multi}\cite{chien2011multi} try to store multiple bits within a cell to further increase storage density. Despite the high storage density, the limited bandwidth of RRAM is always a consideration for its usage as on-chip memory. But recent work~\cite{fackenthal201419} shows that even high density array of 32Gb achieves 1GB/s read bandwidth and 200MB/s write bandwidth. In this work, we adopt~\cite{dong2014nvsim} as a tool to generate RRAM modules of different sizes in our experiments.